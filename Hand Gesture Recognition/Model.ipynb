{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Hand Gesture Recognition Models"]},{"cell_type":"markdown","metadata":{},"source":["### Imports and Utils"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T17:00:09.227485Z","iopub.status.busy":"2024-05-15T17:00:09.226683Z","iopub.status.idle":"2024-05-15T17:00:09.237902Z","shell.execute_reply":"2024-05-15T17:00:09.236977Z","shell.execute_reply.started":"2024-05-15T17:00:09.227444Z"},"trusted":true},"outputs":[],"source":["\"\"\"\n","Importing necessary libraries\n","\"\"\"\n","import os\n","import cv2\n","\n","try:\n","    import mediapipe as mp\n","except ImportError:\n","    %pip install mediapipe\n","    import mediapipe as mp\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from sklearn.model_selection import train_test_split\n","\n","# Remove all the warnings\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Set env CUDA_LAUNCH_BLOCKING=1\n","os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Initialize MediaPipe Imports\n","mp_drawings = mp.solutions.drawing_utils\n","mp_hands = mp.solutions.hands"]},{"cell_type":"code","execution_count":2,"metadata":{"trusted":true},"outputs":[],"source":["\"\"\"\n","Defining Utiliy Functions\n","\"\"\"\n","\n","def landmarks_to_list(image, multi_hand_landmarks) -> torch.Tensor:\n","    \"\"\"\n","    image: The image on which the landmarks are detected\n","    multi_hand_landmarks: The landmarks of the hand\n","\n","    Returns: torch.Tensor: The tensor of landmarks [x, y]\n","    \"\"\"\n","\n","    image_height, image_width, _ = image.shape\n","\n","    landmarks_list = []\n","    if multi_hand_landmarks:\n","        for hand_landmarks in multi_hand_landmarks:\n","            for landmark in hand_landmarks.landmark:\n","                # orgin is top left corner (0, 0)\n","                landmarks_list.append([min(int(landmark.x * image_width), image_width - 1), -min(int(landmark.y * image_height), image_height - 1)])\n","\n","    return torch.tensor(landmarks_list)\n","\n","def normalize_landmarks(landmarks: torch.Tensor) -> torch.Tensor:\n","    \"\"\"\n","    landmarks: torch.Tensor: The landmarks of the hand\n","\n","    Returns: torch.Tensor: The normalized landmarks\n","    \"\"\"\n","\n","    landmarks = landmarks - landmarks[0]\n","    landmarks = landmarks / torch.max(landmarks)\n","\n","    return landmarks"]},{"cell_type":"markdown","metadata":{},"source":["### Dataset Creation and Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Define the dataset\n","dataset = []"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Initialize MediaPipe Hands\n","HandLandmarker = mp_hands.Hands(\n","    static_image_mode=False,\n","    max_num_hands=2,\n","    min_detection_confidence=0.8,\n","    min_tracking_confidence=0.5\n",")\n","\n","# Initialize the webcam\n","cap = cv2.VideoCapture(0)\n","label = 0\n","count = 0\n","\n","with HandLandmarker as landmarker:\n","    while cap.isOpened():\n","        success, frame = cap.read()\n","        if not success:\n","            print(\"Ignoring empty camera frame.\")\n","            continue\n","\n","        # the BGR image to RGB.\n","        frame = cv2.flip(frame, 1)\n","        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","\n","        # Dectect the hand landmarks\n","        frame.flags.writeable = False\n","        results = landmarker.process(frame)\n","\n","        # Draw the hand annotations on the image.\n","        frame.flags.writeable = True\n","        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n","        key = cv2.waitKey(5) & 0xFF\n","        if results.multi_hand_landmarks:\n","            for hand_landmarks in results.multi_hand_landmarks:\n","                mp_drawings.draw_landmarks(\n","                    frame,\n","                    hand_landmarks,\n","                    mp_hands.HAND_CONNECTIONS,\n","                    mp_drawings.DrawingSpec(color=(97, 137, 48), thickness=2, circle_radius=4),\n","                    mp_drawings.DrawingSpec(color=(255, 255, 255), thickness=2, circle_radius=2),\n","                )\n","            \n","            if key == 13:\n","                # Convert the landmarks to a list wrt the image\n","                landmarks = landmarks_to_list(frame, results.multi_hand_landmarks)\n","\n","                # Normalize the landmarks\n","                landmarks = normalize_landmarks(landmarks).flatten()\n","\n","                # Append the label to the landmarks and store in the dataset\n","                dataset.append(torch.cat((landmarks, torch.tensor([label]))))\n","                print(f\"{count} - Gesture Labelled: {label}\")\n","\n","        cv2.imshow('MediaPipe Hands', frame)\n","        if key == 27: # ESC\n","            break\n","\n","cap.release()\n","# cv2.destroyAllWindows()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Convert the dataset to a tensor\n","if dataset != []:\n","    dataset = torch.stack(dataset).to(device)\n","    torch.save(dataset, 'Dataset/dataset.pt')"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T16:52:47.620493Z","iopub.status.busy":"2024-05-15T16:52:47.619977Z","iopub.status.idle":"2024-05-15T16:52:47.634154Z","shell.execute_reply":"2024-05-15T16:52:47.633031Z","shell.execute_reply.started":"2024-05-15T16:52:47.620460Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["X_train shape:  torch.Size([256, 42])\n","X_test shape:  torch.Size([64, 42])\n","Y_train shape:  torch.Size([256, 4])\n","Y_test shape:  torch.Size([64, 4])\n"]}],"source":["# Load the tensor from a file\n","dataset = torch.load('/kaggle/input/dataset/dataset.pt').to(device)\n","\n","# Increase the size of the dataset by 4 times\n","dataset = torch.cat([dataset, dataset, dataset, dataset])\n","\n","# Separate the features and labels\n","features = dataset[:, :-1]\n","labels = dataset[:, -1]\n","labels = F.one_hot(labels.long())\n","\n","# Split the dataset\n","X_train, X_test, Y_train, Y_test = train_test_split(features.cpu().numpy(), labels.cpu().numpy(), test_size=0.2, random_state=42)\n","\n","# Convert back to tensors\n","X_train = torch.from_numpy(X_train).to(device)\n","X_test = torch.from_numpy(X_test).to(device)\n","Y_train = torch.from_numpy(Y_train).long().to(device)\n","Y_test = torch.from_numpy(Y_test).long().to(device)\n","\n","# Print the shape of all tensors\n","print(\"X_train shape: \", X_train.shape)\n","print(\"X_test shape: \", X_test.shape)\n","print(\"Y_train shape: \", Y_train.shape)\n","print(\"Y_test shape: \", Y_test.shape)"]},{"cell_type":"markdown","metadata":{},"source":["### Model Creation and Training"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T17:02:05.149138Z","iopub.status.busy":"2024-05-15T17:02:05.148768Z","iopub.status.idle":"2024-05-15T17:02:05.159552Z","shell.execute_reply":"2024-05-15T17:02:05.158616Z","shell.execute_reply.started":"2024-05-15T17:02:05.149109Z"},"trusted":true},"outputs":[],"source":["class HGRModel(nn.Module):\n","    \"\"\"\n","    A MLP Model for Hand Gesture Recognition\n","    \"\"\"\n","    def __init__(self, in_features, out_features):\n","        super(HGRModel, self).__init__()\n","        self.model = nn.Sequential(\n","            nn.Linear(in_features, 16),\n","            nn.ReLU(),\n","            nn.Linear(16, 32),\n","            nn.ReLU(),\n","            nn.Linear(32, out_features),\n","            nn.Softmax(dim=1)  # Add softmax layer\n","        )\n","        \n","    def forward(self, x):\n","        return self.model(x)\n","    \n","    def fit(self, X, Y, epochs=1000, lr=0.01):\n","        \"\"\"\n","        X: torch.Tensor of shape (n_samples, n_features)\n","        Y: torch.Tensor of shape (n_samples, n_channels)\n","        epochs: int, the number of epochs\n","        \"\"\"\n","        criteria = nn.CrossEntropyLoss()\n","        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n","        \n","        for epoch in range(epochs):\n","            # Forward pass\n","            preds = self.forward(X)\n","            loss = criteria(preds, Y)\n","\n","            # Backward pass\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            # Print the loss\n","            if (epoch+1) % 10 == 0:\n","                print(f'Epoch {epoch+1} Loss: {loss.item()}')\n","                print(\"\\n----------------------------------------------------\\n\")\n","\n","    def save_model(self, file_path):\n","        \"\"\"\n","        Save the model to a file.\n","        \"\"\"\n","        torch.save(self.state_dict(), file_path)\n","\n","    def load_model(self, file_path):\n","        \"\"\"\n","        Load the model from a file.\n","        \"\"\"\n","        self.load_state_dict(torch.load(file_path))"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T17:02:07.639689Z","iopub.status.busy":"2024-05-15T17:02:07.639053Z","iopub.status.idle":"2024-05-15T17:02:07.645609Z","shell.execute_reply":"2024-05-15T17:02:07.644691Z","shell.execute_reply.started":"2024-05-15T17:02:07.639660Z"},"trusted":true},"outputs":[],"source":["# Initialize the model\n","model = HGRModel(X_train.shape[1], Y_train.shape[1]).to(device)"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T17:02:11.278955Z","iopub.status.busy":"2024-05-15T17:02:11.277790Z","iopub.status.idle":"2024-05-15T17:02:11.385542Z","shell.execute_reply":"2024-05-15T17:02:11.384658Z","shell.execute_reply.started":"2024-05-15T17:02:11.278906Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 10 Loss: 1.132049322128296\n","\n","----------------------------------------------------\n","\n","Epoch 20 Loss: 0.8226261138916016\n","\n","----------------------------------------------------\n","\n","Epoch 30 Loss: 0.7468557953834534\n","\n","----------------------------------------------------\n","\n","Epoch 40 Loss: 0.7439714074134827\n","\n","----------------------------------------------------\n","\n","Epoch 50 Loss: 0.7437458634376526\n","\n","----------------------------------------------------\n","\n","Training Accuracy: 100.0%\n"]}],"source":["# Train the model\n","model.fit(X_train, torch.argmax(Y_train, axis=1), epochs=50, lr=0.01)\n","\n","# Evaluate the model\n","model.eval()\n","\n","with torch.no_grad():\n","    Y_pred = model(X_train)\n","    Y_pred = torch.argmax(Y_pred, axis=1)\n","\n","    accuracy_train = (Y_pred == torch.argmax(Y_train, axis=1)).sum().item() / Y_train.shape[0]\n","\n","print(f'Training Accuracy: {accuracy_train * 100}%')"]},{"cell_type":"markdown","metadata":{},"source":["### Testing and Saving"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T17:02:17.070710Z","iopub.status.busy":"2024-05-15T17:02:17.069772Z","iopub.status.idle":"2024-05-15T17:02:17.077670Z","shell.execute_reply":"2024-05-15T17:02:17.076674Z","shell.execute_reply.started":"2024-05-15T17:02:17.070678Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Testing Accuracy: 100.0%\n"]}],"source":["# Evaluate the model\n","model.eval()\n","\n","with torch.no_grad():\n","    Y_pred = model(X_test)\n","    Y_pred = torch.argmax(Y_pred, axis=1)\n","\n","    accuracy_test = (Y_pred == torch.argmax(Y_test, axis=1)).sum().item() / Y_test.shape[0]\n","\n","print(f'Testing Accuracy: {accuracy_test * 100}%')"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T17:05:37.261326Z","iopub.status.busy":"2024-05-15T17:05:37.260393Z","iopub.status.idle":"2024-05-15T17:05:37.271102Z","shell.execute_reply":"2024-05-15T17:05:37.270208Z","shell.execute_reply.started":"2024-05-15T17:05:37.261291Z"},"trusted":true},"outputs":[],"source":["# Models directory\n","directory = \"Models\"\n","os.makedirs(directory, exist_ok=True)\n","\n","# Save the model\n","file_path = \"Models/HGR_Model.pth\"\n","model.save_model(file_path)"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T17:07:31.528602Z","iopub.status.busy":"2024-05-15T17:07:31.528177Z","iopub.status.idle":"2024-05-15T17:07:31.538345Z","shell.execute_reply":"2024-05-15T17:07:31.537358Z","shell.execute_reply.started":"2024-05-15T17:07:31.528574Z"},"trusted":true},"outputs":[],"source":["# Initialize the model\n","model = HGRModel(X_train.shape[1], Y_train.shape[1]).to(device)\n","\n","# Load the model\n","file_path = \"Models/HGR_Model.pth\"\n","model.load_model(file_path)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# Load the model\n","model_path = \"Models/HGR_Model.pth\"\n","model = HGRModel(42, 4)\n","model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n","model.eval()\n","\n","# Initialize MediaPipe Hands\n","HandLandmarker = mp_hands.Hands(\n","    static_image_mode=False,\n","    max_num_hands=1,\n","    min_detection_confidence=0.8,\n","    min_tracking_confidence=0.5\n",")\n","classes = {\n","    0: \"Right hand open\",\n","    1: \"Left hand open\",\n","    2: \"Right hand close\",\n","    3: \"Left hand close\"\n","}\n","\n","# Initialize the webcam\n","cap = cv2.VideoCapture(0)\n","\n","with HandLandmarker as landmarker:\n","    while cap.isOpened():\n","        success, frame = cap.read()\n","        if not success:\n","            print(\"Ignoring empty camera frame.\")\n","            continue\n","\n","        # Convert the BGR image to RGB.\n","        frame = cv2.flip(frame, 1)\n","        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","\n","        # Detect the hand landmarks\n","        frame.flags.writeable = False\n","        results = landmarker.process(frame)\n","\n","        # Draw the hand annotations on the image.\n","        frame.flags.writeable = True\n","        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n","        key = cv2.waitKey(5) & 0xFF\n","        if results.multi_hand_landmarks:\n","            for hand_landmarks in results.multi_hand_landmarks:\n","                mp_drawings.draw_landmarks(\n","                    frame,\n","                    hand_landmarks,\n","                    mp_hands.HAND_CONNECTIONS,\n","                    mp_drawings.DrawingSpec(color=(97, 137, 48), thickness=2, circle_radius=4),\n","                    mp_drawings.DrawingSpec(color=(255, 255, 255), thickness=2, circle_radius=2),\n","                )\n","\n","                # Convert the landmarks to a list wrt the image\n","                landmarks = landmarks_to_list(frame, results.multi_hand_landmarks)\n","\n","                # Normalize the landmarks\n","                landmarks = normalize_landmarks(landmarks).reshape(1, -1)\n","                Y_pred = model(landmarks)\n","                pred_class = torch.argmax(Y_pred, axis=1).item()\n","\n","                # Annotate the predicted class on the screen\n","                cv2.putText(frame, classes[pred_class], (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n","\n","        cv2.imshow('MediaPipe Hands', frame)\n","        if key == 27: # ESC\n","            break\n","\n","cap.release()\n","cv2.destroyAllWindows()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5014610,"sourceId":8422732,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
