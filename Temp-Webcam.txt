    def VideoTransformer(frame):
        img = frame.to_ndarray(format="bgr24")

        with HandLandmarker as landmarker:
            # Convert the BGR image to RGB.
            img = cv2.flip(img, 1)
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

            # Detect the hand landmarks
            img.flags.writeable = False
            results = landmarker.process(img)

            # Draw the hand annotations on the image.
            img.flags.writeable = True
            img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
            if results.multi_hand_landmarks:
                for hand_landmarks in results.multi_hand_landmarks:
                    mp_drawings.draw_landmarks(
                        img,
                        hand_landmarks,
                        mp_hands.HAND_CONNECTIONS,
                        mp_drawings.DrawingSpec(color=(97, 137, 48), thickness=2, circle_radius=4),
                        mp_drawings.DrawingSpec(color=(255, 255, 255), thickness=2, circle_radius=2),
                    )

                    # Convert the landmarks to a list wrt the image
                    landmarks = landmarks_to_list(img, results.multi_hand_landmarks)

                    # Normalize the landmarks
                    landmarks = normalize_landmarks(landmarks).reshape(1, -1)
                    Y_pred = model(landmarks)
                    pred_class = torch.argmax(Y_pred, axis=1).item()

                    # Annotate the predicted class on the screen
                    cv2.putText(img, classes[pred_class], (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)

                return av.VideoFrame.from_ndarray(img, format="bgr24")

    webrtc_streamer(
        key="example",
        media_stream_constraints={"video": True, "audio": False},
        video_frame_callback=VideoTransformer,
    )


    if 'run' not in st.session_state:
        st.session_state.run = False

    cols = st.columns(4)
    if st.session_state.run:
        st.session_state.run = not st.button('Stop')
    else:
        st.session_state.run = st.button('Run')

    FRAME_WINDOW = st.empty()
    cap = cv2.VideoCapture(0)

    if st.session_state.run:
        with HandLandmarker as landmarker:
            while st.session_state.run:
                success, frame = cap.read()
                if not success:
                    continue

                # Convert the BGR image to RGB.
                frame = cv2.flip(frame, 1)
                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

                # Detect the hand landmarks
                frame.flags.writeable = False
                results = landmarker.process(frame)

                # Draw the hand annotations on the image.
                frame.flags.writeable = True
                frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
                if results.multi_hand_landmarks:
                    for hand_landmarks in results.multi_hand_landmarks:
                        mp_drawings.draw_landmarks(
                            frame,
                            hand_landmarks,
                            mp_hands.HAND_CONNECTIONS,
                            mp_drawings.DrawingSpec(color=(97, 137, 48), thickness=2, circle_radius=4),
                            mp_drawings.DrawingSpec(color=(255, 255, 255), thickness=2, circle_radius=2),
                        )

                        # Convert the landmarks to a list wrt the image
                        landmarks = landmarks_to_list(frame, results.multi_hand_landmarks)

                        # Normalize the landmarks
                        landmarks = normalize_landmarks(landmarks).reshape(1, -1)
                        Y_pred = model(landmarks)
                        pred_class = torch.argmax(Y_pred, axis=1).item()

                        # Annotate the predicted class on the screen
                        cv2.putText(frame, classes[pred_class], (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)

                FRAME_WINDOW.image(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))

        cap.release()