{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Super Resolution Models"]},{"cell_type":"markdown","metadata":{},"source":["### Imports and Utils"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T17:00:09.227485Z","iopub.status.busy":"2024-05-15T17:00:09.226683Z","iopub.status.idle":"2024-05-15T17:00:09.237902Z","shell.execute_reply":"2024-05-15T17:00:09.236977Z","shell.execute_reply.started":"2024-05-15T17:00:09.227444Z"},"trusted":true},"outputs":[],"source":["\"\"\"\n","Importing necessary libraries\n","\"\"\"\n","import os\n","import torch\n","import torchvision\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn import preprocessing\n","from sklearn.kernel_approximation import RBFSampler\n","\n","# Remove all the warnings\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Set env CUDA_LAUNCH_BLOCKING=1\n","os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","try:\n","    from einops import rearrange\n","except ImportError:\n","    %pip install einops\n","    from einops import rearrange"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create Coordinate Map\n","def create_coordinate_map(img, scale):\n","    \"\"\"\n","    img: torch.Tensor of shape (num_channels, height, width)\n","    scale: int, the scale factor for the image\n","    \n","    Return: tuple of torch.Tensor of shape (height * width * scale**2, 2) and torch.Tensor of shape (height * width * scale**2, num_channels)\n","    \"\"\"\n","    \n","    # Upscale the image\n","    num_channels, height, width = img.shape\n","    \n","    # Create a 2D grid of (x,y) coordinates (h, w)\n","    w_coords = torch.arange(0, width, 1/scale).repeat(int(height*scale), 1)\n","    h_coords = torch.arange(0, height, 1/scale).repeat(int(width*scale), 1).t()\n","    w_coords = w_coords.reshape(-1)\n","    h_coords = h_coords.reshape(-1)\n","\n","    # Combine the x and y coordinates into a single tensor\n","    X = torch.stack([h_coords, w_coords], dim=1).float()\n","\n","    # Reshape the image to (h * w, num_channels)\n","    Y = torch.tensor(rearrange(img, 'c h w -> (h w) c').float())\n","\n","    # Move X to GPU if available\n","    X = X.to(device)\n","    Y = Y.to(device)\n","    return X, Y"]},{"cell_type":"code","execution_count":2,"metadata":{"trusted":true},"outputs":[],"source":["\"\"\"\n","Making Functions for Plotting and Comparing\n","\"\"\"\n","\n","def plot_compare_two_images(img1, img2, title1='Image 1', title2='Image 2'):\n","    \"\"\"\n","    Plots a comparison between two images in a subplot.\n","    \"\"\"\n","\n","    fig, ax = plt.subplots(1, 2, figsize=(15, 10))\n","\n","    ax[0].imshow(rearrange(img1, 'c h w -> h w c').numpy())\n","    ax[0].set_title(title1)\n","    ax[0].axis('off')\n","\n","    ax[1].imshow(rearrange(img2, 'c h w -> h w c').numpy())\n","    ax[1].set_title(title2)\n","    ax[1].axis('off')\n","\n","    plt.show()\n","\n","def plot_compare_three_images(img1, img2, img3, title1='Image 1', title2='Image 2', title3='Image 3'):\n","    \"\"\"\n","    Plots a comparison between two images in a subplot.\n","    \"\"\"\n","    \n","    fig, ax = plt.subplots(1, 3, figsize=(20, 7))\n","\n","    ax[0].imshow(rearrange(img1, 'c h w -> h w c').numpy())\n","    ax[0].set_title(title1)\n","    ax[0].axis('off')\n","\n","    ax[1].imshow(rearrange(img2, 'c h w -> h w c').numpy())\n","    ax[1].set_title(title2)\n","    ax[1].axis('off')\n","\n","    ax[2].imshow(rearrange(img3, 'c h w -> h w c').numpy())\n","    ax[2].set_title(title3)\n","    ax[2].axis('off')\n","\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### Dataset Creation and Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Set the path to the image\n","path = './Dataset/dog.jpg'\n","\n","# Load the image\n","if not os.path.exists(path):\n","    !wget https://segment-anything.com/assets/gallery/AdobeStock_94274587_welsh_corgi_pembroke_CD.jpg -O {path}\n","\n","# Read in a image from torchvision\n","img = torchvision.io.read_image(path)\n","print(f\"Original Image Shape: {img.shape}\")\n","plt.imshow(rearrange(img, 'c h w -> h w c').numpy())\n","plt.title('Original Image')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Normalize the image\n","scaler_img = preprocessing.MinMaxScaler().fit(img.reshape(-1, 1))\n","img = torch.tensor(scaler_img.transform(img.reshape(-1, 1)).reshape(img.shape))\n","img = img.to(device)\n","print(f'Image shape: {img.shape}')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Crop the image\n","img_cropped = torchvision.transforms.functional.crop(img.cpu(), 600, 800, 400, 400)\n","img_cropped = img_cropped.to(device)\n","print(f'Image cropped shape: {img_cropped.shape}')\n","plt.imshow(rearrange(img_cropped, 'c h w -> h w c').cpu().numpy())\n","plt.title('Cropped Image')\n","plt.show()"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T16:52:47.620493Z","iopub.status.busy":"2024-05-15T16:52:47.619977Z","iopub.status.idle":"2024-05-15T16:52:47.634154Z","shell.execute_reply":"2024-05-15T16:52:47.633031Z","shell.execute_reply.started":"2024-05-15T16:52:47.620460Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["X_train shape:  torch.Size([256, 42])\n","X_test shape:  torch.Size([64, 42])\n","Y_train shape:  torch.Size([256, 4])\n","Y_test shape:  torch.Size([64, 4])\n"]}],"source":["# Create the coordinate map\n","X, Y = create_coordinate_map(img_cropped, scale=1)\n","print(f'X shape: {X.shape}, Y shape: {Y.shape}')\n","\n","# MinMaxScaler\n","minmax = preprocessing.MinMaxScaler(feature_range=(-1, 1)).fit(X.cpu())\n","X_scaled = minmax.transform(X.cpu())\n","\n","# Move the scaled X coordinates to the GPU\n","X_scaled = torch.tensor(X_scaled).to(device).float()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["num_features = 15000\n","sigma = 0.008\n","\n","# Transform the X coordinates to the new map space\n","RFF = RBFSampler(n_components=num_features, gamma=1/(2 * sigma**2))\n","X_RFF = RFF.fit_transform(X.cpu().numpy())\n","X_RFF = torch.tensor(X_RFF, dtype=torch.float32).to(device)"]},{"cell_type":"markdown","metadata":{},"source":["### Model Creation and Training"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T17:02:05.149138Z","iopub.status.busy":"2024-05-15T17:02:05.148768Z","iopub.status.idle":"2024-05-15T17:02:05.159552Z","shell.execute_reply":"2024-05-15T17:02:05.158616Z","shell.execute_reply.started":"2024-05-15T17:02:05.149109Z"},"trusted":true},"outputs":[],"source":["class SuperResModel(nn.Module):\n","    \"\"\"\n","    A Linear Regression Model for Super Resolution\n","    \"\"\"\n","    def __init__(self, in_features, out_features):\n","        super(SuperResModel, self).__init__()\n","        \"\"\"\n","        \n","        \"\"\"\n","        self.linear = nn.Linear(in_features, out_features)\n","        \n","    def forward(self, x):\n","        return self.linear(x)\n","    \n","    def fit(self, X, Y, epochs=10000, lr=0.01):\n","        \"\"\"\n","        X: torch.Tensor of shape (n_samples, n_features)\n","        Y: torch.Tensor of shape (n_samples, n_channels)\n","        epochs: int, the number of epochs\n","        \"\"\"\n","        criteria = nn.MSELoss()\n","        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n","        \n","        for epoch in range(epochs):\n","            # Forward pass\n","            preds = self.forward(X)\n","            loss = criteria(preds, Y)\n","\n","            # Backward pass\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            # Print the loss\n","            if (epoch+1) % 1000 == 0:\n","                print(f'Epoch {epoch+1} Loss: {loss.item()}')\n","                print(\"\\n----------------------------------------------------\\n\")\n","\n","    def save_model(self, file_path):\n","        \"\"\"\n","        Save the model to a file.\n","        \"\"\"\n","        torch.save(self.state_dict(), file_path)\n","\n","    def load_model(self, file_path):\n","        \"\"\"\n","        Load the model from a file.\n","        \"\"\"\n","        self.load_state_dict(torch.load(file_path))"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T17:02:07.639689Z","iopub.status.busy":"2024-05-15T17:02:07.639053Z","iopub.status.idle":"2024-05-15T17:02:07.645609Z","shell.execute_reply":"2024-05-15T17:02:07.644691Z","shell.execute_reply.started":"2024-05-15T17:02:07.639660Z"},"trusted":true},"outputs":[],"source":["# Initialize the model\n","model = SuperResModel(X_RFF.shape[1], Y.shape[1]).to(device)"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T17:02:11.278955Z","iopub.status.busy":"2024-05-15T17:02:11.277790Z","iopub.status.idle":"2024-05-15T17:02:11.385542Z","shell.execute_reply":"2024-05-15T17:02:11.384658Z","shell.execute_reply.started":"2024-05-15T17:02:11.278906Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 10 Loss: 1.132049322128296\n","\n","----------------------------------------------------\n","\n","Epoch 20 Loss: 0.8226261138916016\n","\n","----------------------------------------------------\n","\n","Epoch 30 Loss: 0.7468557953834534\n","\n","----------------------------------------------------\n","\n","Epoch 40 Loss: 0.7439714074134827\n","\n","----------------------------------------------------\n","\n","Epoch 50 Loss: 0.7437458634376526\n","\n","----------------------------------------------------\n","\n","Training Accuracy: 100.0%\n"]}],"source":["# Train the model\n","model.fit(X_RFF, Y, epochs=50000, lr=0.008)"]},{"cell_type":"markdown","metadata":{},"source":["### Generating Super Resolution"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T17:02:17.070710Z","iopub.status.busy":"2024-05-15T17:02:17.069772Z","iopub.status.idle":"2024-05-15T17:02:17.077670Z","shell.execute_reply":"2024-05-15T17:02:17.076674Z","shell.execute_reply.started":"2024-05-15T17:02:17.070678Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Testing Accuracy: 100.0%\n"]}],"source":["# Create the coordinate map\n","X, Y = create_coordinate_map(img_cropped, scale=2)\n","\n","# MinMaxScaler\n","X_scaled = minmax.transform(X.cpu())\n","\n","# Move the scaled X coordinates to the GPU\n","X_scaled = torch.tensor(X_scaled).to(device).float()\n","\n","# Transform the X coordinates to the new map space\n","X_RFF = RFF.transform(X.cpu().numpy())\n","X_RFF = torch.tensor(X_RFF, dtype=torch.float32).to(device)"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T17:05:37.261326Z","iopub.status.busy":"2024-05-15T17:05:37.260393Z","iopub.status.idle":"2024-05-15T17:05:37.271102Z","shell.execute_reply":"2024-05-15T17:05:37.270208Z","shell.execute_reply.started":"2024-05-15T17:05:37.261291Z"},"trusted":true},"outputs":[],"source":["# Extract the super resolution image\n","img_super = model(X_RFF).detach()\n","img_super = rearrange(img_super, '(h w) c -> c h w', h=400)"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T17:07:31.528602Z","iopub.status.busy":"2024-05-15T17:07:31.528177Z","iopub.status.idle":"2024-05-15T17:07:31.538345Z","shell.execute_reply":"2024-05-15T17:07:31.537358Z","shell.execute_reply.started":"2024-05-15T17:07:31.528574Z"},"trusted":true},"outputs":[],"source":["# Plot the comparison\n","plot_compare_two_images(img_cropped.cpu(), img_super.cpu(), title1='Original Image', title2='Super Resolution Image')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot the zoomed in comparison\n","img_cropped_cropped = torchvision.transforms.functional.crop(img_cropped.cpu(), 0, 0, 100, 100)\n","img_super_cropped = torchvision.transforms.functional.crop(img_super.cpu(), 0, 0, 200, 200)\n","\n","plot_compare_two_images(img_cropped_cropped, img_super_cropped, title1='Original Image', title2='Super Resolution Image')"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5014610,"sourceId":8422732,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":4}
