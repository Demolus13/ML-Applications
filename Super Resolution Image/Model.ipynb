{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Super Resolution Models"]},{"cell_type":"markdown","metadata":{},"source":["### Imports and Utils"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T17:00:09.227485Z","iopub.status.busy":"2024-05-15T17:00:09.226683Z","iopub.status.idle":"2024-05-15T17:00:09.237902Z","shell.execute_reply":"2024-05-15T17:00:09.236977Z","shell.execute_reply.started":"2024-05-15T17:00:09.227444Z"},"trusted":true},"outputs":[],"source":["\"\"\"\n","Importing necessary libraries\n","\"\"\"\n","import os\n","import torch\n","import torchvision\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn import preprocessing\n","from sklearn.kernel_approximation import RBFSampler\n","\n","# Remove all the warnings\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Set env CUDA_LAUNCH_BLOCKING=1\n","os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","try:\n","    from einops import rearrange\n","except ImportError:\n","    %pip install einops\n","    from einops import rearrange"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create Coordinate Map\n","def create_coordinate_map(img, scale):\n","    \"\"\"\n","    img: torch.Tensor of shape (num_channels, height, width)\n","    scale: int, the scale factor for the image\n","    \n","    Return: tuple of torch.Tensor of shape (height * width * scale**2, 2) and torch.Tensor of shape (height * width * scale**2, num_channels)\n","    \"\"\"\n","    \n","    # Upscale the image\n","    num_channels, height, width = img.shape\n","    \n","    # Create a 2D grid of (x,y) coordinates (h, w)\n","    w_coords = torch.arange(0, width, 1/scale).repeat(int(height*scale), 1)\n","    h_coords = torch.arange(0, height, 1/scale).repeat(int(width*scale), 1).t()\n","    w_coords = w_coords.reshape(-1)\n","    h_coords = h_coords.reshape(-1)\n","\n","    # Combine the x and y coordinates into a single tensor\n","    X = torch.stack([h_coords, w_coords], dim=1).float()\n","\n","    # Reshape the image to (h * w, num_channels)\n","    Y = torch.tensor(rearrange(img, 'c h w -> (h w) c').float())\n","\n","    # Move X to GPU if available\n","    X = X.to(device)\n","    Y = Y.to(device)\n","    return X, Y"]},{"cell_type":"code","execution_count":2,"metadata":{"trusted":true},"outputs":[],"source":["\"\"\"\n","Making Functions for Plotting and Comparing\n","\"\"\"\n","\n","def plot_compare_two_images(img1, img2, title1='Image 1', title2='Image 2', main_title='Comparison'):\n","    \"\"\"\n","    Plots a comparison between two images in a subplot.\n","    \"\"\"\n","\n","    fig, ax = plt.subplots(1, 2, figsize=(15, 10))\n","\n","    ax[0].imshow(rearrange(img1, 'c h w -> h w c').numpy())\n","    ax[0].set_title(title1)\n","    ax[0].axis('off')\n","\n","    ax[1].imshow(rearrange(img2, 'c h w -> h w c').numpy())\n","    ax[1].set_title(title2)\n","    ax[1].axis('off')\n","\n","    fig.suptitle(main_title)\n","    plt.show()\n","\n","def plot_compare_three_images(img1, img2, img3, title1='Image 1', title2='Image 2', title3='Image 3', main_title='Comparison'):\n","    \"\"\"\n","    Plots a comparison between two images in a subplot.\n","    \"\"\"\n","    \n","    fig, ax = plt.subplots(1, 3, figsize=(20, 7))\n","\n","    ax[0].imshow(rearrange(img1, 'c h w -> h w c').numpy())\n","    ax[0].set_title(title1)\n","    ax[0].axis('off')\n","\n","    ax[1].imshow(rearrange(img2, 'c h w -> h w c').numpy())\n","    ax[1].set_title(title2)\n","    ax[1].axis('off')\n","\n","    ax[2].imshow(rearrange(img3, 'c h w -> h w c').numpy())\n","    ax[2].set_title(title3)\n","    ax[2].axis('off')\n","\n","    fig.suptitle(main_title)\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### Dataset Creation and Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Set the path to the image\n","path = './Dataset/dog.jpg'\n","\n","# Load the image\n","if not os.path.exists(path):\n","    !wget https://segment-anything.com/assets/gallery/AdobeStock_94274587_welsh_corgi_pembroke_CD.jpg -O {path}\n","\n","# Read in a image from torchvision\n","img = torchvision.io.read_image(path)\n","print(f\"Original Image Shape: {img.shape}\")\n","plt.imshow(rearrange(img, 'c h w -> h w c').numpy())\n","plt.title('Original Image')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Normalize the image\n","scaler_img = preprocessing.MinMaxScaler().fit(img.reshape(-1, 1))\n","img = torch.tensor(scaler_img.transform(img.reshape(-1, 1)).reshape(img.shape))\n","img = img.to(device)\n","print(f'Image shape: {img.shape}')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Crop the image\n","img_cropped = torchvision.transforms.functional.crop(img.cpu(), 600, 800, 400, 400)\n","img_cropped = img_cropped.to(device)\n","print(f'Image cropped shape: {img_cropped.shape}')\n","plt.imshow(rearrange(img_cropped, 'c h w -> h w c').cpu().numpy())\n","plt.title('Cropped Image')\n","plt.show()"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T16:52:47.620493Z","iopub.status.busy":"2024-05-15T16:52:47.619977Z","iopub.status.idle":"2024-05-15T16:52:47.634154Z","shell.execute_reply":"2024-05-15T16:52:47.633031Z","shell.execute_reply.started":"2024-05-15T16:52:47.620460Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["X_train shape:  torch.Size([256, 42])\n","X_test shape:  torch.Size([64, 42])\n","Y_train shape:  torch.Size([256, 4])\n","Y_test shape:  torch.Size([64, 4])\n"]}],"source":["# Create the coordinate map\n","X, Y = create_coordinate_map(img_cropped, scale=1)\n","print(f'X shape: {X.shape}, Y shape: {Y.shape}')\n","\n","# MinMaxScaler\n","minmax = preprocessing.MinMaxScaler(feature_range=(-1, 1)).fit(X.cpu())\n","X_scaled = minmax.transform(X.cpu())\n","\n","# Move the scaled X coordinates to the GPU\n","X_scaled = torch.tensor(X_scaled).to(device).float()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["num_features = 15000\n","sigma = 0.008\n","\n","# Transform the X coordinates to the new map space\n","RFF = RBFSampler(n_components=num_features, gamma=1/(2 * sigma**2))\n","X_RFF = RFF.fit_transform(X.cpu().numpy())\n","X_RFF = torch.tensor(X_RFF, dtype=torch.float32).to(device)"]},{"cell_type":"markdown","metadata":{},"source":["### Model Creation and Training"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T17:02:05.149138Z","iopub.status.busy":"2024-05-15T17:02:05.148768Z","iopub.status.idle":"2024-05-15T17:02:05.159552Z","shell.execute_reply":"2024-05-15T17:02:05.158616Z","shell.execute_reply.started":"2024-05-15T17:02:05.149109Z"},"trusted":true},"outputs":[],"source":["class SuperResModel(nn.Module):\n","    \"\"\"\n","    A Linear Regression Model for Super Resolution\n","    \"\"\"\n","    def __init__(self, in_features, out_features):\n","        super(SuperResModel, self).__init__()\n","        \"\"\"\n","        \n","        \"\"\"\n","        self.linear = nn.Linear(in_features, out_features)\n","        \n","    def forward(self, x):\n","        return self.linear(x)\n","    \n","    def fit(self, X, Y, epochs=10000, lr=0.01):\n","        \"\"\"\n","        X: torch.Tensor of shape (n_samples, n_features)\n","        Y: torch.Tensor of shape (n_samples, n_channels)\n","        epochs: int, the number of epochs\n","        \"\"\"\n","        criteria = nn.MSELoss()\n","        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n","        \n","        for epoch in range(epochs):\n","            # Forward pass\n","            preds = self.forward(X)\n","            loss = criteria(preds, Y)\n","\n","            # Backward pass\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            # Print the loss\n","            if (epoch+1) % 1000 == 0:\n","                print(f'Epoch {epoch+1} Loss: {loss.item()}')\n","                print(\"\\n----------------------------------------------------\\n\")\n","\n","    def save_model(self, file_path):\n","        \"\"\"\n","        Save the model to a file.\n","        \"\"\"\n","        torch.save(self.state_dict(), file_path)\n","\n","    def load_model(self, file_path):\n","        \"\"\"\n","        Load the model from a file.\n","        \"\"\"\n","        self.load_state_dict(torch.load(file_path))"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T17:02:07.639689Z","iopub.status.busy":"2024-05-15T17:02:07.639053Z","iopub.status.idle":"2024-05-15T17:02:07.645609Z","shell.execute_reply":"2024-05-15T17:02:07.644691Z","shell.execute_reply.started":"2024-05-15T17:02:07.639660Z"},"trusted":true},"outputs":[],"source":["# Initialize the model\n","model = SuperResModel(X_RFF.shape[1], Y.shape[1]).to(device)"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T17:02:11.278955Z","iopub.status.busy":"2024-05-15T17:02:11.277790Z","iopub.status.idle":"2024-05-15T17:02:11.385542Z","shell.execute_reply":"2024-05-15T17:02:11.384658Z","shell.execute_reply.started":"2024-05-15T17:02:11.278906Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 10 Loss: 1.132049322128296\n","\n","----------------------------------------------------\n","\n","Epoch 20 Loss: 0.8226261138916016\n","\n","----------------------------------------------------\n","\n","Epoch 30 Loss: 0.7468557953834534\n","\n","----------------------------------------------------\n","\n","Epoch 40 Loss: 0.7439714074134827\n","\n","----------------------------------------------------\n","\n","Epoch 50 Loss: 0.7437458634376526\n","\n","----------------------------------------------------\n","\n","Training Accuracy: 100.0%\n"]}],"source":["# Train the model\n","model.fit(X_RFF, Y, epochs=50, lr=0.01)"]},{"cell_type":"markdown","metadata":{},"source":["### Testing and Saving"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T17:02:17.070710Z","iopub.status.busy":"2024-05-15T17:02:17.069772Z","iopub.status.idle":"2024-05-15T17:02:17.077670Z","shell.execute_reply":"2024-05-15T17:02:17.076674Z","shell.execute_reply.started":"2024-05-15T17:02:17.070678Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Testing Accuracy: 100.0%\n"]}],"source":["# Evaluate the model\n","model.eval()\n","\n","with torch.no_grad():\n","    Y_pred = model(X_test)\n","    Y_pred = torch.argmax(Y_pred, axis=1)\n","\n","    accuracy_test = (Y_pred == torch.argmax(Y_test, axis=1)).sum().item() / Y_test.shape[0]\n","\n","print(f'Testing Accuracy: {accuracy_test * 100}%')"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T17:05:37.261326Z","iopub.status.busy":"2024-05-15T17:05:37.260393Z","iopub.status.idle":"2024-05-15T17:05:37.271102Z","shell.execute_reply":"2024-05-15T17:05:37.270208Z","shell.execute_reply.started":"2024-05-15T17:05:37.261291Z"},"trusted":true},"outputs":[],"source":["# Models directory\n","directory = \"Models\"\n","os.makedirs(directory, exist_ok=True)\n","\n","# Save the model\n","file_path = \"Models/HGR_Model.pth\"\n","model.save_model(file_path)"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T17:07:31.528602Z","iopub.status.busy":"2024-05-15T17:07:31.528177Z","iopub.status.idle":"2024-05-15T17:07:31.538345Z","shell.execute_reply":"2024-05-15T17:07:31.537358Z","shell.execute_reply.started":"2024-05-15T17:07:31.528574Z"},"trusted":true},"outputs":[],"source":["# Initialize the model\n","model = HGRModel(X_train.shape[1], Y_train.shape[1]).to(device)\n","\n","# Load the model\n","file_path = \"Models/HGR_Model.pth\"\n","model.load_model(file_path)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# Load the model\n","model_path = \"Models/HGR_Model.pth\"\n","model = HGRModel(42, 4)\n","model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n","model.eval()\n","\n","# Initialize MediaPipe Hands\n","HandLandmarker = mp_hands.Hands(\n","    static_image_mode=False,\n","    max_num_hands=1,\n","    min_detection_confidence=0.8,\n","    min_tracking_confidence=0.5\n",")\n","classes = {\n","    0: \"Right hand open\",\n","    1: \"Left hand open\",\n","    2: \"Right hand close\",\n","    3: \"Left hand close\"\n","}\n","\n","# Initialize the webcam\n","cap = cv2.VideoCapture(0)\n","\n","with HandLandmarker as landmarker:\n","    while cap.isOpened():\n","        success, frame = cap.read()\n","        if not success:\n","            print(\"Ignoring empty camera frame.\")\n","            continue\n","\n","        # Convert the BGR image to RGB.\n","        frame = cv2.flip(frame, 1)\n","        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","\n","        # Detect the hand landmarks\n","        frame.flags.writeable = False\n","        results = landmarker.process(frame)\n","\n","        # Draw the hand annotations on the image.\n","        frame.flags.writeable = True\n","        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n","        key = cv2.waitKey(5) & 0xFF\n","        if results.multi_hand_landmarks:\n","            for hand_landmarks in results.multi_hand_landmarks:\n","                mp_drawings.draw_landmarks(\n","                    frame,\n","                    hand_landmarks,\n","                    mp_hands.HAND_CONNECTIONS,\n","                    mp_drawings.DrawingSpec(color=(97, 137, 48), thickness=2, circle_radius=4),\n","                    mp_drawings.DrawingSpec(color=(255, 255, 255), thickness=2, circle_radius=2),\n","                )\n","\n","                # Convert the landmarks to a list wrt the image\n","                landmarks = landmarks_to_list(frame, results.multi_hand_landmarks)\n","\n","                # Normalize the landmarks\n","                landmarks = normalize_landmarks(landmarks).reshape(1, -1)\n","                Y_pred = model(landmarks)\n","                pred_class = torch.argmax(Y_pred, axis=1).item()\n","\n","                # Annotate the predicted class on the screen\n","                cv2.putText(frame, classes[pred_class], (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n","\n","        cv2.imshow('MediaPipe Hands', frame)\n","        if key == 27: # ESC\n","            break\n","\n","cap.release()\n","cv2.destroyAllWindows()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5014610,"sourceId":8422732,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":4}
